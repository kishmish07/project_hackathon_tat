# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MA8-gaG3AprtB2Ylpx1NgFOaBJk8_mth
"""

import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from datasets import Dataset, DatasetDict
from accelerate import Accelerator
from transformers import set_seed
import random
import numpy as np
import pandas as pd
import evaluate


SEED = 42
set_seed(SEED)

DATA_PATH = "/content/data2.0.csv"
SUBMIT_PATH = "/content/test_inputs.tsv"
SEP = ","
TOXIC_COL = "toxic"
DETOX_COL = "detox"

MODEL_NAME = "google/mt5-base"
TASK_PREFIX = "Rewrite the following Tatar sentence in polite and non-toxic form while preserving the exact meaning: "

MAX_SOURCE_LENGTH = 128
MAX_TARGET_LENGTH = 128

BATCH_SIZE = 1
NUM_EPOCHS = 3

df = pd.read_csv(DATA_PATH, sep=SEP)
df = df[[TOXIC_COL, DETOX_COL]].dropna().reset_index(drop=True)

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

dataset = DatasetDict({"train": train_dataset, "test": test_dataset})


tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

model.gradient_checkpointing_enable()
model.config.use_cache = False

tokenizer.pad_token = "<pad>"
model.config.pad_token_id = tokenizer.pad_token_id


def preprocess_function(examples):
    model_inputs = tokenizer(
        [TASK_PREFIX + t for t in examples[TOXIC_COL]],
        max_length=MAX_SOURCE_LENGTH,
        truncation=True,
    )

    labels = tokenizer(
        examples[DETOX_COL],
        max_length=MAX_TARGET_LENGTH,
        truncation=True,
    )["input_ids"]

    model_inputs["labels"] = labels
    return model_inputs

tokenized = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=[TOXIC_COL, DETOX_COL]
)

print("Tokenized train:", len(tokenized["train"]))
print("Tokenized test:", len(tokenized["test"]))


bleu = evaluate.load("sacrebleu")
rouge = evaluate.load("rouge")


accelerator = Accelerator()

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding="max_length",
    max_length=MAX_SOURCE_LENGTH,
    label_pad_token_id=-100,
    return_tensors="pt",
)
train_loader = DataLoader(
    tokenized["train"],
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=data_collator
)

eval_loader = DataLoader(
    tokenized["test"],
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=data_collator
)

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

model, optimizer, train_loader, eval_loader = accelerator.prepare(
    model, optimizer, train_loader, eval_loader
)

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0.0
    steps = 0

    for batch in train_loader:

        optimizer.zero_grad()

        outputs = model(**batch)

        loss = outputs.loss

        if not torch.isfinite(loss):
            accelerator.print("⚠️ пропуск (loss is nan/inf)")
            continue

        accelerator.backward(loss)

        accelerator.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()

        total_loss += loss.item()
        steps += 1

    if steps == 0:
        accelerator.print(f"⚠️ Epoch {epoch+1}: все лоссы nan — попробуй fp32")
    else:
        avg_loss = total_loss / steps
        accelerator.print(f"Epoch {epoch+1} loss: {avg_loss:.4f}")




preds = []
refs = []

unwrapped = accelerator.unwrap_model(model)
unwrapped.eval()

for batch in eval_loader:
    with torch.no_grad():
        generated = unwrapped.generate(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            max_length=MAX_TARGET_LENGTH,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=2
        )

    decoded_preds = tokenizer.batch_decode(generated.cpu(), skip_special_tokens=True)

    labels = batch["labels"].detach().cpu().numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    preds.extend(decoded_preds)
    refs.extend(decoded_labels)

print("BLEU:", bleu.compute(predictions=preds, references=[[r] for r in refs]))
print("ROUGE:", rouge.compute(predictions=preds, references=refs))

def generate_detox(sentence: str, num_beams=4):
    inp = TASK_PREFIX + sentence
    enc = tokenizer(
        inp,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_SOURCE_LENGTH,
    ).to(accelerator.device)

    unwrapped = accelerator.unwrap_model(model)
    unwrapped.eval()

    with torch.no_grad():
        out_ids = unwrapped.generate(
            **enc,
            max_length=MAX_TARGET_LENGTH,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7,
            no_repeat_ngram_size=3,
        )

    result = tokenizer.decode(out_ids[0], skip_special_tokens=True)
    if not result.strip():
        result = sentence

    return result


submit_df = pd.read_csv(SUBMIT_PATH, sep="\t")

preds = []
for text in submit_df["tat_toxic"]:
    detoxed = generate_detox(text)
    preds.append(detoxed)

final_df = pd.DataFrame({
    "ID": submit_df["ID"],
    "tat_toxic": submit_df["tat_toxic"],
    "tat_detox1": preds,
})

OUTPUT_FILE = "/content/submission.tsv"
final_df.to_csv(OUTPUT_FILE, sep="\t", index=False)
!zip -j /content/submission.zip /content/submission.tsv

print("\nПримеры:\n")

for i in range(20):
    idx = random.randint(0, len(test_df) - 1)
    toxic = test_df.iloc[idx][TOXIC_COL]
    detox_ref = test_df.iloc[idx][DETOX_COL]
    detox_pred = generate_detox(toxic)

    if detox_pred.strip() != toxic.strip():
        print("Токсичное:      ", toxic)
        print("Эталонный ответ:", detox_ref)
        print("Модель:         ", detox_pred)
        print("-" * 80)

!pip install transformers==4.38.2 accelerate datasets evaluate sentencepiece sacrebleu rouge-score