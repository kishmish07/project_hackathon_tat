
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from datasets import Dataset, DatasetDict
from accelerate import Accelerator
from transformers import set_seed
import random
import numpy as np
import pandas as pd
import evaluate
from sklearn.model_selection import train_test_split

SEED = 42
set_seed(SEED)

TRAIN_PATH = "/content/test.csv"
SUBMIT_PATH = "/content/dev_inputs.tsv"
MODEL_NAME = "bigscience/mt0-small"

TASK_PREFIX = "Detoxify the following Tatar sentence while preserving the meaning: "

TOXIC_COL = "toxic"
DETOX_COL = "detox"

MAX_SOURCE_LENGTH = 128
MAX_TARGET_LENGTH = 128
BATCH_SIZE = 8
NUM_EPOCHS = 3

df = pd.read_csv(TRAIN_PATH)
df = df[[TOXIC_COL, DETOX_COL]].dropna().reset_index(drop=True)

train_df, test_df = train_test_split(df, test_size=0.3, random_state=SEED)

train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id


def preprocess_function(examples):
    inputs = [TASK_PREFIX + t for t in examples[TOXIC_COL]]
    targets = examples[DETOX_COL]

    model_inputs = tokenizer(
        inputs,
        max_length=MAX_SOURCE_LENGTH,
        truncation=True,
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=MAX_TARGET_LENGTH,
            truncation=True,
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

accelerator = Accelerator()

data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    padding="longest",
    return_tensors="pt",
)

train_loader = DataLoader(tokenized["train"], batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=data_collator)
eval_loader = DataLoader(tokenized["test"], batch_size=BATCH_SIZE, shuffle=False,
                         collate_fn=data_collator)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

model, optimizer, train_loader, eval_loader = accelerator.prepare(
    model, optimizer, train_loader, eval_loader
)

print("Device:", accelerator.device)

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()
        outputs = model(**batch)
        loss = outputs.loss

        accelerator.backward(loss)
        optimizer.step()

        total_loss += loss.item()

    accelerator.print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

def generate_detox(sentence: str, num_beams: int = 4) -> str:
    inp = TASK_PREFIX + sentence
    enc = tokenizer(
        inp,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_SOURCE_LENGTH,
    ).to(accelerator.device)

    unwrapped = accelerator.unwrap_model(model)
    unwrapped.eval()
    with torch.no_grad():
        out_ids = unwrapped.generate(
            **enc,
            max_length=MAX_TARGET_LENGTH,
            num_beams=num_beams,
            early_stopping=True,
            no_repeat_ngram_size=2,
        )
    result = tokenizer.decode(out_ids[0], skip_special_tokens=True)
    if not result.strip():
        result = sentence

    return result


submit_df = pd.read_csv(SUBMIT_PATH, sep="\t")

preds = []
for text in submit_df["tat_toxic"]:
    detoxed = generate_detox(text)
    preds.append(detoxed)

final_df = pd.DataFrame({
    "ID": submit_df["ID"],
    "tat_toxic": submit_df["tat_toxic"],
    "tat_detox1": preds,
})

OUTPUT_FILE = "/content/submission.tsv"
final_df.to_csv(OUTPUT_FILE, sep="\t", index=False)
!zip /content/submission.zip /content/submission.tsv
