# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ccm93iBQRhUBkCEp5I3eTj0Cp5rmWGA
"""

!pip install transformers datasets accelerate sentencepiece sacrebleu

import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"



from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv(list(uploaded.keys())[0])
df.head()

from datasets import Dataset

dataset = Dataset.from_pandas(df)

dataset = dataset.rename_columns({
    "toxic": "source",
    "detox": "target"
})

dataset = dataset.train_test_split(test_size=0.1)

train_dataset = dataset["train"]
val_dataset = dataset["test"]

train_dataset, val_dataset

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "bigscience/mt0-small"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

from transformers import DataCollatorForSeq2Seq

def preprocess(example):
    inputs = tokenizer(
        example["input"],
        max_length=128,
        padding="max_length",
        truncation=True
    )

    labels = tokenizer(
        example["output"],
        max_length=128,
        padding="max_length",
        truncation=True
    )

    inputs["labels"] = labels["input_ids"]
    return inputs

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./mt0-detox",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_steps=50,
    save_steps=500,
    eval_strategy="epoch",
    learning_rate=2e-4,
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=True,  # Работает на GPU
)

train_dataset = train_dataset.map(preprocess)
val_dataset = val_dataset.map(preprocess)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    train_dataset=train_dataset,
   eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

trainer.save_model("mt0-detox-model")
tokenizer.save_pretrained("mt0-detox-model")

def detox(text):
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True)

detox("Мин сине сөйли торган әйткән сүзләрең бөтенләй ямьсез!")